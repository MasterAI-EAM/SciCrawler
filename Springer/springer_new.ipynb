{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method #1 (Save to XML file, limesoup cleaning needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from io import BytesIO\n",
    "\n",
    "def clean_filepath(filepath):\n",
    "    cleaned_filepath = filepath.replace(\"//\", \"/\")\n",
    "    while \"//\" in cleaned_filepath:\n",
    "        cleaned_filepath = cleaned_filepath.replace(\"//\", \"/\")\n",
    "    return cleaned_filepath\n",
    "\n",
    "def request_article(doi):\n",
    "    key = ''\n",
    "    base_url = 'https://spdi.public.springernature.app/xmldata/jats'\n",
    "    url = f\"{base_url}?q=doi:{doi}&api_key={key}/unsw-api\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, features=\"lxml\")\n",
    "    title = str(soup.find('total'))\n",
    "    if title == '<total>0</total>':\n",
    "        return False\n",
    "    else:\n",
    "        return response.content\n",
    "\n",
    "def gather_existing_dois(output_folder):\n",
    "    existing_dois = set()\n",
    "    for root, dirs, files in os.walk(output_folder):\n",
    "        for file in files:\n",
    "            if file.endswith(\".xml\"):\n",
    "                doi = file.replace(\".xml\", \"\").replace(\"-\", \"/\")\n",
    "                existing_dois.add(doi)\n",
    "    return existing_dois\n",
    "\n",
    "def process_json_files(main_folder, output_folder, fail_record_folder):\n",
    "    existing_dois = gather_existing_dois(output_folder)\n",
    "    fail_record_file_path = os.path.join(fail_record_folder, \"fail_record.json\")\n",
    "    failed_dois = set()\n",
    "    \n",
    "    if os.path.exists(fail_record_file_path):\n",
    "        with open(fail_record_file_path, 'r') as fail_record_file:\n",
    "            fail_record = json.load(fail_record_file)\n",
    "            failed_dois = set(fail_record.get(\"failed_dois\", []))\n",
    "\n",
    "    for root, dirs, files in os.walk(main_folder):\n",
    "        for file in tqdm(files, desc=\"Processing JSON files\", unit=\"file\"):\n",
    "            if file.endswith(\".json\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r') as json_file:\n",
    "                    data = json.load(json_file)\n",
    "\n",
    "                # Extract DOIs\n",
    "                for entry in data.values():\n",
    "                    doi = entry[\"doi\"]\n",
    "                    \n",
    "                    # Skip if DOI has been processed or exists\n",
    "                    if doi in existing_dois or doi in failed_dois:\n",
    "                        print(f\"Skipping {doi}, already processed.\")\n",
    "                        continue\n",
    "\n",
    "                    # Check if the file already exists in the output folder\n",
    "                    doi_for_filename = doi.replace(\"/\", \"-\")\n",
    "                    output_file_path = os.path.join(output_folder, root[len(main_folder) + 1:], f\"{doi_for_filename}.xml\")\n",
    "\n",
    "                    # Clean up the output file path\n",
    "                    cleaned_output_file_path = clean_filepath(output_file_path)\n",
    "\n",
    "                    # Create the output folder if it doesn't exist\n",
    "                    output_subfolder = os.path.dirname(cleaned_output_file_path)\n",
    "                    os.makedirs(output_subfolder, exist_ok=True)\n",
    "\n",
    "                    article_downloaded = request_article(doi)\n",
    "\n",
    "                    if not article_downloaded:\n",
    "                        # Mark DOI as failed\n",
    "                        failed_dois.add(doi)\n",
    "                    else:\n",
    "                        # Save the XML data directly to file\n",
    "                        with open(cleaned_output_file_path, 'wb') as file:\n",
    "                            file.write(article_downloaded)\n",
    "\n",
    "    # Save the fail record after processing all files\n",
    "    fail_record = {\"failed_dois\": list(failed_dois)}\n",
    "    with open(clean_filepath(fail_record_file_path), 'w') as fail_record_file:\n",
    "        json.dump(fail_record, fail_record_file, indent=2)\n",
    "\n",
    "# Specify the main folder path, output folder path, and fail record folder path\n",
    "main_folder_path = \"path to folder containing json metadata files\"\n",
    "output_folder_path = \"path to output folder\"\n",
    "fail_record_folder_path = \"\"\n",
    "\n",
    "# Create the fail record folder if it doesn't exist\n",
    "os.makedirs(fail_record_folder_path, exist_ok=True)\n",
    "\n",
    "# Call the main function\n",
    "process_json_files(main_folder_path, output_folder_path, fail_record_folder_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method #2 save to json-contented XML file, xml2json conversion needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from io import BytesIO\n",
    "from chemdataextractor.doc import Document, Heading, Paragraph\n",
    "from chemdataextractor.reader import NlmXmlReader\n",
    "\n",
    "def sanitize_filepath(filepath):\n",
    "    # Replace backslashes with slashes\n",
    "    filepath = filepath.replace(\"\\\\\", \"/\")\n",
    "    # Replace colons with dots\n",
    "    filepath = filepath.replace(\":\", \".\")\n",
    "    return filepath\n",
    "\n",
    "def clean_filepath(filepath):\n",
    "    # First, sanitize the filepath to replace invalid characters\n",
    "    sanitized_filepath = sanitize_filepath(filepath)\n",
    "    # Ensure there are no redundant slashes\n",
    "    cleaned_filepath = sanitized_filepath.replace(\"//\", \"/\")\n",
    "    while \"//\" in cleaned_filepath:\n",
    "        cleaned_filepath = cleaned_filepath.replace(\"//\", \"/\")\n",
    "    return cleaned_filepath\n",
    "\n",
    "def request_article(doi):\n",
    "    key = ''\n",
    "    base_url = 'https://spdi.public.springernature.app/xmldata/jats'\n",
    "    url = base_url + '?q=doi:' + doi + '&api_key=' + key + '/unsw-api'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, features=\"lxml\")\n",
    "    title = str(soup.find('total'))\n",
    "    if title == '<total>0</total>':\n",
    "        return False\n",
    "    else:\n",
    "        return response.content\n",
    "\n",
    "def gather_existing_dois(output_folder):\n",
    "    existing_dois = set()\n",
    "    for root, dirs, files in os.walk(output_folder):\n",
    "        for file in files:\n",
    "            if file.endswith(\".xml\"):\n",
    "                doi = file.replace(\".xml\", \"\").replace(\"-\", \"/\")\n",
    "                existing_dois.add(doi)\n",
    "    return existing_dois\n",
    "\n",
    "def process_json_files(main_folder, output_folder, fail_record_folder):\n",
    "    existing_dois = gather_existing_dois(output_folder)\n",
    "    fail_record_file_path = os.path.join(fail_record_folder, \"fail_record.json\")\n",
    "    failed_dois = set()\n",
    "    if os.path.exists(fail_record_file_path):\n",
    "        with open(fail_record_file_path, 'r') as fail_record_file:\n",
    "            fail_record = json.load(fail_record_file)\n",
    "            failed_dois = set(fail_record.get(\"failed_dois\", []))\n",
    "\n",
    "    for root, dirs, files in os.walk(main_folder):\n",
    "        for file in tqdm(files, desc=\"Processing JSON files\", unit=\"file\"):\n",
    "            if file.endswith(\".json\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r') as json_file:\n",
    "                    data = json.load(json_file)\n",
    "\n",
    "                # Extract dois\n",
    "                for entry in data.values():\n",
    "                    doi = entry[\"doi\"]\n",
    "                    \n",
    "                    # Skip if DOI has been processed or exists\n",
    "                    if doi in existing_dois or doi in failed_dois:\n",
    "                        print(f\"Skipping {doi}, already processed.\")\n",
    "                        continue\n",
    "\n",
    "                    # Check if the file already exists in the output folder\n",
    "                    doi_for_filename = doi.replace(\"/\", \"-\")\n",
    "                    output_file_path = os.path.join(output_folder, root[len(main_folder) + 1:], f\"{doi_for_filename}.xml\")\n",
    "\n",
    "                    # Clean up the output file path\n",
    "                    cleaned_output_file_path = clean_filepath(output_file_path)\n",
    "\n",
    "                    # Create the output folder if it doesn't exist\n",
    "                    output_subfolder = os.path.dirname(cleaned_output_file_path)\n",
    "                    os.makedirs(output_subfolder, exist_ok=True)\n",
    "\n",
    "                    article_downloaded = request_article(doi)\n",
    "\n",
    "                    if not article_downloaded:\n",
    "                        # Mark DOI as failed\n",
    "                        failed_dois.add(doi)\n",
    "                    else:\n",
    "                        r = BytesIO(article_downloaded)\n",
    "                        try:\n",
    "                            doc = Document.from_file(r, readers=[NlmXmlReader()])\n",
    "                        except:\n",
    "                            print('Failed to read DOI:', doi)\n",
    "                            # Mark DOI as failed\n",
    "                            failed_dois.add(doi)\n",
    "                            continue\n",
    "\n",
    "                        # Store as txt\n",
    "                        store_json = {\"Sections\": []}\n",
    "                        tmp = {}\n",
    "                        tmp[\"name\"] = \"\"\n",
    "                        tmp[\"content\"] = []\n",
    "\n",
    "                        for e in doc.elements:\n",
    "                            if isinstance(e, Heading):\n",
    "                                if tmp != {} and tmp[\"content\"] != []:\n",
    "                                    store_json[\"Sections\"].append(tmp)\n",
    "                                tmp = {}\n",
    "                                tmp[\"name\"] = str(e)\n",
    "                                tmp[\"content\"] = []\n",
    "                            if isinstance(e, Paragraph):\n",
    "                                word_num = len(str(e).split(' '))\n",
    "                                if word_num > 100:\n",
    "                                    str_e = str(e).replace('/n', '')\n",
    "                                    if not str_e.startswith(\"Open Access\"):\n",
    "                                        tmp[\"content\"].append(str_e)\n",
    "\n",
    "                        json_str = json.dumps(store_json, indent=4)\n",
    "                        with open(cleaned_output_file_path, 'w', encoding='utf-8') as json_file:\n",
    "                            json_file.write(json_str)\n",
    "\n",
    "    # Save the fail record after processing all files\n",
    "    fail_record = {\"failed_dois\": list(failed_dois)}\n",
    "    with open(clean_filepath(fail_record_file_path), 'w') as fail_record_file:\n",
    "        json.dump(fail_record, fail_record_file, indent=2)\n",
    "\n",
    "# Specify the main folder path, output folder path, and fail record folder path\n",
    "main_folder_path = \"\"\n",
    "output_folder_path = \"\"\n",
    "fail_record_folder_path = \"\" # \n",
    "\n",
    "# Create the fail record folder if it doesn't exist\n",
    "os.makedirs(fail_record_folder_path, exist_ok=True)\n",
    "\n",
    "# Call the main function\n",
    "process_json_files(main_folder_path, output_folder_path, fail_record_folder_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method #3 save to all content file in one key & value, as json files, no cleaning needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "def request_article(doi):\n",
    "    try:\n",
    "        key = ''\n",
    "        base_url = 'https://spdi.public.springernature.app/xmldata/jats'\n",
    "        url = base_url + '?q=doi:' + doi + '&api_key=' + key + '/unsw-api'\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx)\n",
    "        soup = BeautifulSoup(response.content, features=\"lxml\")\n",
    "        title = str(soup.find('total'))\n",
    "        if title == '<total>0</total>':\n",
    "            return False\n",
    "        else:\n",
    "            return soup.find('body').get_text()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading content for DOI {doi}: {e}\")\n",
    "        return False\n",
    "\n",
    "def process_json_files(main_folder, output_folder, fail_record_folder):\n",
    "    for root, dirs, files in os.walk(main_folder):\n",
    "        for file in tqdm(files, desc=\"Processing JSON files\", unit=\"file\"):\n",
    "            if file.endswith(\".json\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r') as json_file:\n",
    "                    data = json.load(json_file)\n",
    "\n",
    "                for doi, content in tqdm(data.items(), desc=\"Processing DOIs\", unit=\"doi\"):\n",
    "                    print(f\"Processing DOI: {doi}\")\n",
    "\n",
    "                    # Check if the file already exists in the output folder\n",
    "                    doi_for_filename = doi.replace(\"/\", \"-\")\n",
    "                    output_file_path = os.path.join(output_folder, root[len(main_folder) + 1:], f\"{doi_for_filename}.json\")\n",
    "\n",
    "                    if os.path.exists(output_file_path):\n",
    "                        print(f\"Skipping {doi}, file already exists.\")\n",
    "                        continue\n",
    "\n",
    "                    article_text = request_article(doi)\n",
    "\n",
    "                    if article_text:\n",
    "                        # Create a new folder structure for storing downloaded content\n",
    "                        output_subfolder = os.path.join(output_folder, root[len(main_folder) + 1:])\n",
    "                        os.makedirs(output_subfolder, exist_ok=True)\n",
    "\n",
    "                        # Save downloaded content to a new JSON file\n",
    "                        output_file_path = os.path.join(output_subfolder, f\"{doi_for_filename}.json\")\n",
    "                        with open(output_file_path, 'w') as output_json_file:\n",
    "                            json.dump({\"doi\": doi, \"downloaded_text\": article_text}, output_json_file, indent=2)\n",
    "                    else:\n",
    "                        # Save DOI to the fail record\n",
    "                        fail_record_file_path = os.path.join(fail_record_folder, \"fail_record.json\")\n",
    "                        fail_record = {\"failed_dois\": []}\n",
    "\n",
    "                        if os.path.exists(fail_record_file_path):\n",
    "                            with open(fail_record_file_path, 'r') as fail_record_file:\n",
    "                                fail_record = json.load(fail_record_file)\n",
    "\n",
    "                        fail_record[\"failed_dois\"].append(doi)\n",
    "\n",
    "                        with open(fail_record_file_path, 'w') as fail_record_file:\n",
    "                            json.dump(fail_record, fail_record_file, indent=2)\n",
    "\n",
    "# Specify the main folder path, output folder path, and fail record folder path\n",
    "main_folder_path = ''\n",
    "output_folder_path = ''\n",
    "fail_record_folder_path = ''\n",
    "\n",
    "# Call the main function\n",
    "process_json_files(main_folder_path, output_folder_path, fail_record_folder_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
